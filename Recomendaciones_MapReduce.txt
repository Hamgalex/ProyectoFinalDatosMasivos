import requests
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer, util
import ast
import os
import torch
import multiprocessing
import collections
import itertools

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
EMBEDDING_FILE = os.path.join(BASE_DIR, "books_with_embeddings.csv")

# Cargar modelo
model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

class SimpleMapReduce(object):
    def __init__(self, map_func, reduce_func, num_workers=None):
        self.map_func = map_func
        self.reduce_func = reduce_func
        self.pool = multiprocessing.Pool(num_workers)

    def partition(self, mapped_values):
        partitioned_data = collections.defaultdict(list)
        for key, value in mapped_values:
            partitioned_data[key].append(value)
        return partitioned_data.items()

    def __call__(self, inputs, chunksize=1):
        map_responses = self.pool.map(self.map_func, inputs, chunksize=chunksize)
        partitioned_data = self.partition(itertools.chain(*map_responses))
        reduced_values = self.pool.map(self.reduce_func, partitioned_data)
        return reduced_values

def map_similarity(book_tuple):
    idx, book_emb, query_emb = book_tuple
    sim = util.cos_sim(torch.tensor(query_emb), torch.tensor(book_emb))[0].item()
    return [("similarities", (idx, sim))]

def reduce_top_similarities(item, top_n=5):
    key, values = item
    values.sort(key=lambda x: x[1], reverse=True)
    return values[:top_n]

# Cargar base local
def load_embedding_database():
    if os.path.exists(EMBEDDING_FILE):
        df = pd.read_csv(EMBEDDING_FILE)
        df["embedding"] = df["embedding"].apply(lambda x: np.array(ast.literal_eval(x)))
        print(f" Base cargada con {len(df)} libros.")
        return df
    else:
        print(" No se encontró archivo local. Se usará base vacía.")
        columns = ["title", "author", "categories", "description", "language",
                   "pageCount", "averageRating", "ratingsCount", "publisher",
                   "publishedDate", "embedding"]
        return pd.DataFrame(columns=columns)

# Buscar libro en Google Books
def search_google_books(title):
    params = {"q": title, "maxResults": 1}
    response = requests.get("https://www.googleapis.com/books/v1/volumes", params=params)
    data = response.json()

    if "items" not in data:
        return None

    info = data["items"][0]["volumeInfo"]
    return {
        "title": info.get("title", ""),
        "author": ", ".join(info.get("authors", [])),
        "categories": ", ".join(info.get("categories", [])),
        "description": info.get("description", ""),
        "language": info.get("language", ""),
        "pageCount": info.get("pageCount", ""),
        "averageRating": info.get("averageRating", ""),
        "ratingsCount": info.get("ratingsCount", ""),
        "publisher": info.get("publisher", ""),
        "publishedDate": info.get("publishedDate", "")
    }

# Generar embedding de descripción
def generate_embedding(text):
    return model.encode(text, convert_to_numpy=True)

# Guardar nuevo libro a base local
def add_to_database(df, book, emb):
    # Evitar agregar si ya existe (por título y autor)
    exists = ((df["title"].str.lower() == book["title"].lower()) & 
              (df["author"].str.lower() == book["author"].lower())).any()
    if exists:
        print("El libro ya está en la base.")
        return df

    new_row = book.copy()
    new_row["embedding"] = emb.tolist()

    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
    df["embedding"] = df["embedding"].apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)
    df.to_csv(EMBEDDING_FILE, index=False, encoding="utf-8-sig")
    #print("Nuevo libro agregado a la base.")
    return df
  
# Recomendador principal
def recommend_books_mapreduce(query_title, df, top_n=5, save_new=True):
    print(f"\n Buscando: {query_title}")
    book = search_google_books(query_title)
    if not book or not book["description"]:
        print(" Libro no encontrado o sin descripción.")
        return

    print(f" Título: {book['title']} \n Autor: {book['author']}\n Descripción: {book['description'][:150]}...\n")
    emb = generate_embedding(book["description"])

    if save_new:
        df = add_to_database(df, book, emb)

    if len(df) == 0:
        print(" No hay libros en la base para comparar.")
        return

    query_index = len(df) - 1
    emb_matrix = np.vstack(df["embedding"].values).astype(np.float32)

    # Preparar entradas para Map
    inputs = [(i, emb_matrix[i], emb) for i in range(len(df)) if i != query_index]

    # Ejecutar MapReduce
    mr = SimpleMapReduce(map_similarity, lambda x: reduce_top_similarities(x, top_n=top_n))
    results = mr(inputs)

    print(f"\n Recomendaciones similares:\n")
    for i, sim in results[0]:  # solo hay una clave "similarities"
        print(f" {df.loc[i, 'title']} — {df.loc[i, 'author']}")
        print(f"    Similitud: {sim:.4f}\n")

    return df

# MAIN
if __name__ == "__main__":
    base_df = load_embedding_database()
    while True:
        query = input("\n Ingresa el título del libro (o 'salir'): ")
        if query.lower() == "salir":
            break
        #base_df = recommend_books(query, base_df, top_n=5, save_new=True)
        base_df = recommend_books_mapreduce(query, base_df, top_n=5, save_new=True)